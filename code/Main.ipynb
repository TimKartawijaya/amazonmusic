{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the functions and libraries necessary for this report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run analysis_functions.ipynb #import all helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import main dataset\n",
    "df = pd.read_csv('lastfm_9000_users.csv', na_filter=False)\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the analysis will be as such: \n",
    "1. Objective\n",
    "2. Prepare data\n",
    "3. Model Fitting, Tuning, and Evaluation\n",
    "    - Benchmarks\n",
    "        - Most Popular\n",
    "        - ALS Matrix Factorization\n",
    "    - LightFM\n",
    "        - Vanilla FM / BPR\n",
    "        - FM with User/Item Side Information\n",
    "        - Parameter Tuning\n",
    "4. Model Exploration\n",
    "    - Metrics Used (NDCG, Recall, Precision, Coverage)\n",
    "    - Performance of each Model (Table)\n",
    "    - NDCG Metric by User Type \n",
    "        - Active/Non-Active (Aggregate Plays)\n",
    "        - Diverse/Non-Diverse (Top 1000 Artists)\n",
    "        - \"Basic\"/Non-\"Basic\" (Popular)\n",
    "    - Scale:\n",
    "        - NDCG by Size\n",
    "        - Training Time / Predict Time\n",
    "6. Conclusion / Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the plays are distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6571: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x112e47208>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.distplot(df.plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6571: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x112e47208>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.distplot(df[df.plays < 1000].plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "df['log_plays'] = df.plays.apply(lambda x: log(x))\n",
    "\n",
    "df.log_plays *= (1.0/df.log_plays.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6571: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x112e47208>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.distplot(df.log_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLALBALBALBLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Create Sparse Matrix from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:24: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Sparsity: 99.8965986346416\n"
     ]
    }
   ],
   "source": [
    "#create sparse matrix\n",
    "plays_sparse = create_sparse_matrix(df).astype('float')\n",
    "print('Matrix Sparsity:', calculate_sparsity(plays_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Split Data to Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train test set, maintaining that each user still has some interactions intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec38239e2df4ee885603291a71b3878"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of original data masked: 0.06548419166887459\n",
      "Users masked: 8980\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "train, test, user_count = split_train_test_per_user(plays_sparse, 3, 10)\n",
    "print(\"Percentage of original data masked:\", pct_masked(plays_sparse, train.T.tocsr()))\n",
    "print(\"Users masked:\", user_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<47102x9000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 438337 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train is item by user to accomodate implicit and baseline training\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_log = df.drop(columns = ['plays'], axis = 1)\n",
    "# plays_sparse = create_sparse_matrix(df).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.columns = ['user_id','artist_mbid','artist_name','plays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:24: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Sparsity: 99.89754244924725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f80eb2db7d40398d1d922ae82836fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of original data masked: 0.06580552072418709\n",
      "Users masked: 8939\n"
     ]
    }
   ],
   "source": [
    "#create sparse matrix\n",
    "plays_sparse_log = create_sparse_matrix(df_log).astype('float')\n",
    "print('Matrix Sparsity:', calculate_sparsity(plays_sparse_log))\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_log, test_log, user_count_log = split_train_test_per_user(plays_sparse_log, 3, 10)\n",
    "print(\"Percentage of original data masked:\", pct_masked(plays_sparse_log, train_log.T.tocsr()))\n",
    "print(\"Users masked:\", user_count_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Fitting, Tuning, and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note on evaluation: use metrics\n",
    "- what autotune does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline recommends the most-popular artists to everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting baseline...\n",
      "[2.69867e+05 3.85490e+04 6.92960e+04 ... 2.80000e+01 2.70000e+01\n",
      " 2.40000e+01]\n"
     ]
    }
   ],
   "source": [
    "model_baseline = Baseline(n_recs = 20)\n",
    "model_baseline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tuning is necessary since there are no parameters. We then evaluate the test set below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4e645fda6241e89a0673ee5bd7ba71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 1.175946547884187 %\n",
      "Recall: 7.839643652561247 %\n",
      "Coverage: 0.04246104199397053 %\n",
      "Average NDCG per User: 4.700608311766834 %\n"
     ]
    }
   ],
   "source": [
    "coverage, precision, recall, ndcg = evaluate(model_baseline, \"baseline\", test, plays_sparse)\n",
    "print(\"Precision:\",precision*100,'%')\n",
    "print(\"Recall:\",recall*100,'%')\n",
    "print(\"Coverage:\",coverage*100,'%')\n",
    "print(\"Average NDCG per User:\",ndcg*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Based (ALS)\n",
    "\n",
    "Here we fit the model-based ALS Matrix Factorization using the implicit package from Homework 2 and use the parameters that were found to be optimized in the HW 2 report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15.0/15 [00:04<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "model_als = implicit.als.AlternatingLeastSquares(factors = 30, regularization = 0.01)\n",
    "\n",
    "# Train model\n",
    "print(\"Fitting model...\")\n",
    "model_als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef86dc28f47a4db69e1e2c69059a7ea8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 3.2165924276169267 %\n",
      "Recall: 21.443949517446175 %\n",
      "Coverage: 8.44337820050104 %\n",
      "Average NDCG per User: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "coverage, precision, recall, ndcg = evaluate(model_als, \"implicit\", test, train.T.tocsr())\n",
    "print(\"Precision:\",precision*100,'%')\n",
    "print(\"Recall:\",recall*100,'%')\n",
    "print(\"Coverage:\",coverage*100,'%')\n",
    "print(\"Average NDCG per User:\",ndcg*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightFM (without side information)\n",
    "\n",
    "Here we fit the LightFM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x1139dbdd8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "\n",
    "model_fm_vanilla = LightFM(learning_rate = .05, loss='warp')\n",
    "\n",
    "# Train Model\n",
    "print(\"Fitting model...\")\n",
    "model_fm_vanilla.fit(train_log.T.tocsr(),user_features = None, item_features = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to find the best hyperparameter for this model and use the model with the best hyperparameter to get our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfca128db264f81ab7911952e81ae52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 0.0022373867322966774 %\n",
      "Recall: 0.014915911548644517 %\n",
      "Coverage: 2.201605027387372 %\n",
      "Average NDCG per User: 0.007813011545850596 %\n"
     ]
    }
   ],
   "source": [
    "#coverage, precision, recall, ndcg = evaluate(model_fm_vanilla, \"lightfm\", test, plays_sparse,liked=train.T.tocsr())\n",
    "coverage, precision, recall, ndcg = evaluate(model_fm_vanilla, \"lightfm\", test_log, plays_sparse_log, liked = train_log)\n",
    "print(\"Precision:\",precision*100,'%')\n",
    "print(\"Recall:\",recall*100,'%')\n",
    "print(\"Coverage:\",coverage*100,'%')\n",
    "print(\"Average NDCG per User:\",ndcg*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07283183133668553"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from lightfm.cross_validation import random_train_test_split\n",
    "# from lightfm.evaluation import precision_at_k\n",
    "# from lightfm.evaluation import recall_at_k\n",
    "\n",
    "# train, test = random_train_test_split(plays_sparse_log)\n",
    "# recall_at_k(model_fm_vanilla, test, k = 20).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightFM (with side information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run artist_features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#insert side info here\n",
    "user_features = None\n",
    "item_features = None\n",
    "\n",
    "model_fm_features = LightFM(no_components=30, loss='warp')\n",
    "\n",
    "# Train Model\n",
    "print(\"Fitting model...\")\n",
    "model_fm_features.fit(train.T.tocsr(),user_features, item_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to find the best hyperparameter for thi smodel with the best hyperparameter to get our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Performance Results (Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we explore these models even more. How do they perform in regards to size, different user population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Performance by User Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run grouping_functions.ipynb\n",
    "import sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active/Non-Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=3\n",
    "activity_groups=active_users(plays_sparse, n)\n",
    "levels=[1,2,3]\n",
    "ndcg_activity = []\n",
    "recall_activity = []\n",
    "model = implicit.als.AlternatingLeastSquares(factors=30, regularization=0.01)\n",
    "for group in activity_groups:\n",
    "    print('Matrix Sparsity:', calculate_sparsity(group))\n",
    "    # Compute the recall and ndcg using optimal parameters for the ALS model on different dataset sizes\n",
    "    train, test, user_count = split_train_test_per_user(group, 4, 20)\n",
    "    # train model \n",
    "    print(\"Fitting model...\")\n",
    "    \n",
    "    model.fit(train, show_progress=True)\n",
    "    \n",
    "    recall, ndcg = evaluate(model, test, plays_sparse)\n",
    "    print(\"Recall:\",recall*100,'%')\n",
    "    print(\"Average NDCG:\",ndcg*100,'%')\n",
    "    recall_activity.append(recall)\n",
    "    ndcg_activity.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot scalability of ALS model\n",
    "ndcg_activity_df = pd.DataFrame({'N':levels, 'NDCG': ndcg_activity})\n",
    "g = sns.pointplot(x='N', y='NDCG', data=ndcg_activity_df)\n",
    "plt.title('NDCG by Activity Level for ALS')\n",
    "plt.xlabel('Activity Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diverse/Non-Diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=3\n",
    "diversity_groups=diverse_users(plays_sparse, n)\n",
    "levels=[1,2,3]\n",
    "ndcg_div = []\n",
    "recall_div = []\n",
    "model = implicit.als.AlternatingLeastSquares(factors=30, regularization=0.01)\n",
    "for group in diversity_groups:\n",
    "    print('Matrix Sparsity:', calculate_sparsity(group))\n",
    "    # Compute the recall and ndcg using optimal parameters for the ALS model on different dataset sizes\n",
    "    train, test, user_count = split_train_test_per_user(group, 4, 20)\n",
    "    # train model \n",
    "    print(\"Fitting model...\")\n",
    "    \n",
    "    model.fit(train, show_progress=True)\n",
    "    \n",
    "    recall, ndcg = evaluate(model, test, plays_sparse)\n",
    "    print(\"Recall:\",recall*100,'%')\n",
    "    print(\"Average NDCG:\",ndcg*100,'%')\n",
    "    recall_div.append(recall)\n",
    "    ndcg_div.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot scalability of ALS model\n",
    "ndcg_div_df = pd.DataFrame({'N':levels, 'NDCG': ndcg_div})\n",
    "g = sns.pointplot(x='N', y='NDCG', data=ndcg_div_df)\n",
    "plt.title('NDCG by Diversity Level for ALS')\n",
    "plt.xlabel('Diversity Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic/Non-Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=3\n",
    "mainstream_groups=mainstream_users(plays_sparse, n)\n",
    "levels=[1,2,3]\n",
    "ndcg_mainstream = []\n",
    "recall_mainstream = []\n",
    "model = implicit.als.AlternatingLeastSquares(factors=30, regularization=0.01)\n",
    "for group in mainstream_groups:\n",
    "    print('Matrix Sparsity:', calculate_sparsity(group))\n",
    "    # Compute the recall and ndcg using optimal parameters for the ALS model on different dataset sizes\n",
    "    train, test, user_count = split_train_test_per_user(group, 4, 20)\n",
    "    # train model \n",
    "    print(\"Fitting model...\")\n",
    "    \n",
    "    model.fit(train, show_progress=True)\n",
    "    \n",
    "    recall, ndcg = evaluate(model, test, plays_sparse)\n",
    "    print(\"Recall:\",recall*100,'%')\n",
    "    print(\"Average NDCG:\",ndcg*100,'%')\n",
    "    recall_mainstream.append(recall)\n",
    "    ndcg_mainstream.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot scalability of ALS model\n",
    "ndcg_mainstream_df = pd.DataFrame({'N':levels, 'NDCG': ndcg_mainstream})\n",
    "g = sns.pointplot(x='N', y='NDCG', data=ndcg_mainstream_df)\n",
    "plt.title('NDCG by Mainstream-ness Level for ALS')\n",
    "plt.xlabel('Mainstream-ness Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Performance by Input Size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy/NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = [9000,20000,40000,60000,80000]\n",
    "\n",
    "#get train data for each input size\n",
    "for users in size:\n",
    "    df = get_users(df_150, users)\n",
    "    plays_sparse = create_sparse_matrix(df)\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    model_fm_features = LightFM(no_components=30, loss='warp')\n",
    "    \n",
    "    start = time.time()\n",
    "    # train model \n",
    "    print(\"Fitting model...\")\n",
    "    model_fm_features.fit(plays_sparse.T.tocsr(),user_features = None, item_features = None)    \n",
    "    stop = time.time()\n",
    "    total = stop-start\n",
    "    train_time.append(total)\n",
    "\n",
    "#plot\n",
    "df = pd.DataFrame({'n':size,'time':train_time})\n",
    "sns.pointplot(x=\"n\",y=\"time\",data=df)\n",
    "plt.title('Training Time by Input Size for KNN')\n",
    "plt.xlabel('Number of Users')\n",
    "plt.ylabel('Time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Predict Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot LightFM training time vs input size\n",
    "#calculate training time for different input sizes for ALS\n",
    "import time\n",
    "train_time = list()\n",
    "\n",
    "#get train data for each input size\n",
    "for users in size:\n",
    "    df = get_users(df_150, users)\n",
    "    plays_sparse = create_sparse_matrix(df)\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    model_fm_features = LightFM(no_components=30, loss='warp')\n",
    "    \n",
    "    start = time.time()\n",
    "    # train model \n",
    "    print(\"Fitting model...\")\n",
    "    model_fm_features.fit(plays_sparse.T.tocsr(),user_features = None, item_features = None)    \n",
    "    stop = time.time()\n",
    "    total = stop-start\n",
    "    train_time.append(total)\n",
    "\n",
    "#plot\n",
    "df = pd.DataFrame({'n':size,'time':train_time})\n",
    "sns.pointplot(x=\"n\",y=\"time\",data=df)\n",
    "plt.title('Training Time by Input Size for KNN')\n",
    "plt.xlabel('Number of Users')\n",
    "plt.ylabel('Time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion / Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and Parameter Tuning (k-fold)\n",
    "\n",
    "In this section, we use k-fold cross validation to tune hyperparameters and evaluate our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fdaa12cdf64ad9bbf1ec00fbdbf4d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation test\n",
    "k=5\n",
    "train_list, test_list, user_count = split_train_test_per_user(plays_sparse,k,20,cross_valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and tune ALS\n",
    "\n",
    "Tuning two parameters: number of latent factors and the regularization factor. \n",
    "\n",
    "For latent factors, we try values [10,20,30,40,50,60]\n",
    "\n",
    "For regularization factors, we try [.01,.03,.05,.07]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model=implicit.als.AlternatingLeastSquares\n",
    "ndcg_list,heatmap_list=auto_tune_parameter(4,20,model,plays_sparse,[10,20,30,40,50,60],[.01,.03,.05,.07],param3=None)\n",
    "stop = time.time()\n",
    "total = stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot heatmap of parameter tuning results\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.heatmap(heatmap_list[2], \n",
    "            xticklabels=['0.01','0.03','0.05','0.07'], \n",
    "            yticklabels=['10','20','30','40','50','60'], \n",
    "            cbar_kws={'label':'NDCG Score'})\n",
    "plt.ylabel(\"Number of Latent Factors\")\n",
    "plt.xlabel(\"Regularization Factor\")\n",
    "plt.title(\"ALS NDCG scores according to Model Parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we analyze the scalability of ALS by looking at datasets with 9k, 20k, 60k, and 150k users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following block imports larger datasets for scaling tests, these expanded CSVs are not available in the repo\n",
    "files9k = pd.read_csv('lastfm_9000_users.csv', na_filter=False)\n",
    "files20k = pd.read_csv('lastfm_20k_users.csv', na_filter=False)\n",
    "files60k = pd.read_csv('lastfm_60k_users.csv', na_filter=False)\n",
    "files150k = pd.read_csv('lastfm_150k_users.csv', na_filter=False)\n",
    "files40k = get_users(files150k, 40000)\n",
    "files = [files9k, files20k, files40k, files60k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the recall and ndcg using optimal parameters for the ALS model on different dataset sizes\n",
    "size = [9000, 20000, 40000, 60000]\n",
    "ndcg_size = []\n",
    "recall_size = []\n",
    "for i in files:\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=30, regularization=0.01)\n",
    "\n",
    "    #create sparse matrix\n",
    "    plays_sparse = create_sparse_matrix(i).astype('float')\n",
    "    print('Matrix Sparsity:', calculate_sparsity(plays_sparse))\n",
    "\n",
    "    train, test, user_count = split_train_test_per_user(plays_sparse, 4, 20)\n",
    "\n",
    "    # train model \n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(train, show_progress=True)\n",
    "\n",
    "    recall, ndcg = evaluate(model, test, plays_sparse)\n",
    "    print(\"Recall:\",recall*100,'%')\n",
    "    print(\"Average NDCG:\",ndcg*100,'%')\n",
    "    recall_size.append(recall)\n",
    "    ndcg_size.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot scalability of ALS model\n",
    "ndcg_size_df = pd.DataFrame({'N':size,\n",
    "                       'NDCG': ndcg_size})\n",
    "g = sns.pointplot(x='N', y='NDCG', data=ndcg_size_df)\n",
    "plt.title('NDCG by Input Size for ALS')\n",
    "plt.xlabel('Number Of Users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we analyze the catalog coverage of the ALS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate catalog coverage of ALS model with optimal parameters\n",
    "model = implicit.als.AlternatingLeastSquares(factors=30, regularization=0.01)\n",
    "model.fit(plays_sparse)\n",
    "users = list(df.user_id.unique())\n",
    "catalog = []\n",
    "for i in range(0,len(users)):\n",
    "    for x,y in model.recommend(i,plays_sparse.T.tocsr(), N=20, filter_already_liked_items=True):\n",
    "        if x not in catalog:\n",
    "            catalog.append(x)\n",
    "print('Catalog Coverage is', len(catalog)/plays_sparse.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune ALS\n",
    "\n",
    "Below we will tune the hyperparameters of ALS from a given range of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=implicit.als.AlternatingLeastSquares\n",
    "ndcg_list,heatmap_list=auto_tune_parameter(4,20,model,plays_sparse,[10,20,30,40,50,60],[.01,.03,.05,.07],param3=None)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.heatmap(heatmap_list[2], xticklabels=['0.01','0.03','0.05','0.07'], yticklabels=['10','20','30','40','50','60'], cbar_kws={'label':'NDCG Score'})\n",
    "plt.ylabel(\"Number of Latent Factors\")\n",
    "plt.xlabel(\"Regularization Factor\")\n",
    "plt.title(\"ALS NDCG scores according to Model Parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Size vs Training Time\n",
    "\n",
    "Below you can run the code using a larger dataset to evaluate performance tiem compared with input tiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper function to grab needed users\n",
    "def get_users(df, n):\n",
    "    sample_userid = df[\"user_id\"].unique()\n",
    "    sample_userid = np.random.choice(sample_userid, size = n, replace = False)\n",
    "\n",
    "    #grab rows with sample user id\n",
    "    df_sample = df[df.user_id.isin(sample_userid)].reset_index(drop = True)\n",
    "\n",
    "    return df_sample\n",
    "\n",
    "#in order to run this analysis, you need to download the 150k data at: \n",
    "#https://www.dropbox.com/s/qd8rnlxsuq0rjll/last_fm_bigger_data.zip?dl=0\n",
    "\n",
    "#read in large dataset\n",
    "df_150 = pd.read_csv('lastfm_150k_users.csv', na_filter=False)\n",
    "df_150 = df_150.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot ALS training time vs input size\n",
    "#calculate training time for different input sizes for ALS\n",
    "import sns\n",
    "import time\n",
    "\n",
    "size = [9000,20000,40000,60000,80000]\n",
    "train_time = list()\n",
    "\n",
    "#get train data for each input size\n",
    "for users in size:\n",
    "    df = get_users(df_150, users)\n",
    "    plays_sparse = create_sparse_matrix(df)\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    model = implicit.als.AlternatingLeastSquares(50)\n",
    "    \n",
    "    start = time.time()\n",
    "    # train model \n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(plays_sparse, show_progress=True)\n",
    "    stop = time.time()\n",
    "    total = stop-start\n",
    "    train_time.append(total)\n",
    "\n",
    "#plot\n",
    "df = pd.DataFrame({'n':size,'time':train_time})\n",
    "sns.pointplot(x=\"n\",y=\"time\",data=df)\n",
    "plt.title('Training Time by Input Size for KNN')\n",
    "plt.xlabel('Number of Users')\n",
    "plt.ylabel('Time (s)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
