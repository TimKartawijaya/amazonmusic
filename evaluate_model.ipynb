{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our goal is to have top K artist recommendations for a user, so that they would \n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import implicit\n",
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "df = pd.read_csv('lastfm_9000_users.csv', na_filter=False)\n",
    "df = df[df.plays > 10] #our threshold of relevance is 10 plays\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "BIG THINGS: \n",
    "- set-up cross-validation\n",
    "- calculate precision/recall/AUC using ALL user-pairs, not only k-recommended values\n",
    "- calculate NDCG \n",
    "- calculate ATOP \n",
    "- calculate speed performance\n",
    "\n",
    "CASES TO EVALUATE:\n",
    "- sparse vs. non-sparse data\n",
    "- threshold of relevance\n",
    "- which users should we hold out (those with > 10/20/30 values?)\n",
    "\n",
    "PERFORMANCE:\n",
    "- speed up split_train_per_user, by figuring out how to access zero values faster\n",
    "- speed up precision, recall evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sparse matrix from dataframe object\n",
    "def create_sparse_matrix(data, user_user = True):\n",
    "    \"\"\"\n",
    "    Creates sparse matrix (csr_matrix) out of pandas dataframe.\n",
    "    \n",
    "    Parameters: \n",
    "    - data: Dataframe of user/artist data\n",
    "    - user_user: determines whether output will be for user-to-user or item-to-item collaborative filtering\n",
    "                 if user_user is True, then rows will be items and columns will be users\n",
    "    \n",
    "    Returns: \n",
    "    - plays_sparse: a sparse csr_matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Creating sparse matrix...\")\n",
    "    #grab unique users/artist IDS\n",
    "    users = list(np.sort(data.user_id.unique()))\n",
    "    artists = list(data.artist_name.unique())\n",
    "    plays = list(data.plays)\n",
    "\n",
    "    # user-user set-up\n",
    "    if (user_user == True):\n",
    "        rows = data.artist_name.astype('category', categories=artists).cat.codes\n",
    "        cols = data.user_id.astype('category', categories=users).cat.codes\n",
    "        plays_sparse = scipy.sparse.csr_matrix((plays, (rows, cols)), shape=(len(artists),len(users)))\n",
    "\n",
    "    #item-item set-up\n",
    "    else:    \n",
    "        rows = data.user_id.astype('category', categories=users).cat.codes\n",
    "        cols = data.artist_name.astype('category', categories=artists).cat.codes\n",
    "        plays_sparse = scipy.sparse.csr_matrix((plays, (rows, cols)), shape=(len(users),len(artists)))\n",
    "        \n",
    "    return plays_sparse\n",
    "\n",
    "#calculate how sparse the matrix is\n",
    "def calculate_sparsity(M):\n",
    "    \"\"\"\n",
    "    Calculates how many \n",
    "    p\n",
    "    \"\"\"\n",
    "    matrix_size = float(M.shape[0]*M.shape[1]) # Number of possible interactions in the matrix\n",
    "    num_plays = len(M.nonzero()[0]) # Number of items interacted with\n",
    "    sparsity = 100*(1 - float(num_plays/matrix_size))\n",
    "    return sparsity\n",
    "\n",
    "#split train, test using all user pairs\n",
    "def make_train_all_user_pairs(data, test_pct):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        data: data set in csr_matrix format\n",
    "        test_pct: percentage to be test set\n",
    "    \"\"\"\n",
    "    #create copies of dataset for training and test data\n",
    "    test = data.copy()\n",
    "    train = data.copy()\n",
    "    \n",
    "    #alter train data, masking/holding-out random user-pair values for some users\n",
    "    nonzero_idx = train.nonzero() #find indices in data where interaction exists\n",
    "    nonzero_pairs = zip(nonzero_idx[0], nonzero_idx[1]) #create pairs of (user, item) index\n",
    "    \n",
    "    #determine how many user-pair values we need to mask, according to test_pct\n",
    "    random.seed(0) #for reproducibility\n",
    "    num_samples = int(np.ceil(test_pct * len(nonzero_pairs)))\n",
    "    samples = random.sample(nonzero_pairs, num_samples) #sample random number of user-item pairs without replacement\n",
    "    \n",
    "    #get user, item row and column indices\n",
    "    user_idx = [index[0] for index in samples] \n",
    "    item_idx = [index[1] for index in samples] \n",
    "    \n",
    "    train[user_idx,item_idx] = 0 #mask the randomly chosen user-item pairs\n",
    "    train.eliminate_zeros() #remove zeros in sparse arrays that was made previously\n",
    "    \n",
    "    return train, test, list(set(user_idx)), samples #output unique list of user rows that were altered\n",
    "\n",
    "#split train, test by user only with k interactions\n",
    "def split_train_test_per_user(data, test_pct, min_interactions = 0):\n",
    "    \"\"\"\n",
    "    Create train matrix with masked values and dictionary of test values \n",
    "    \n",
    "    data: csr_matrix, items-rows\n",
    "    test_pct: percentage of items to mask per user\n",
    "    \"\"\"\n",
    "    #create a copy for training data\n",
    "    train = plays_sparse.copy().T.tocsr() #transpose to make calculation easier\n",
    "\n",
    "    random.seed(0) #for reproducibilitiiy\n",
    "\n",
    "    #dictionary with keys as users and values as a list of item-indexes\n",
    "    #which were masked\n",
    "    test = dict()\n",
    "    \n",
    "    print(\"Splitting train, test data for each user...\")\n",
    "    #for each user in sparse matrix, get random user-item pairs to mask\n",
    "    for user_id in tqdm(range(train.get_shape()[0])):\n",
    "\n",
    "        #find item indices interaction exists\n",
    "        nonzero_idx = train[user_id].nonzero()\n",
    "\n",
    "        #only mask users with enough data (greater than 10 interactions)\n",
    "        if nonzero_idx[1].shape[0] >= min_interactions:\n",
    "            item_idx = nonzero_idx[1] #get item indexes of interactions\n",
    "\n",
    "            #sample random number of item_indexes without replacement\n",
    "            num_samples = int(np.ceil(test_pct * item_idx.shape[0]))\n",
    "            samples = random.sample(item_idx, num_samples) \n",
    "\n",
    "            #append user_id, item_indexes to test dictionary\n",
    "            test[user_id] = samples\n",
    "    \n",
    "            #mask the randomly chosen items of this user\n",
    "            for items in samples:\n",
    "                train[user_id, items] = 0\n",
    "\n",
    "    #remove zeros in sparse arrays to save space\n",
    "    train.eliminate_zeros()\n",
    "    \n",
    "    return train.T.tocsr(), test #convert back\n",
    "\n",
    "#calculate how many interactions are masked compared to previous dataset\n",
    "def pct_masked(original, altered):\n",
    "    altered_n = altered.nonzero()[0].shape[0]\n",
    "    original_n = original.nonzero()[0].shape[0]\n",
    "    return (original_n - altered_n)/float(altered_n)\n",
    "\n",
    "#used to evaluate model\n",
    "def evaluate(model, test, M, n_rec = 20):\n",
    "    \"\"\"\n",
    "    Calculate precision/recall\n",
    "    \n",
    "    parameters:\n",
    "    - model: fitted implicit model that will perform recommendations\n",
    "    - test: dict containing items that are heldout for each user\n",
    "    - M: csr_matrix of item-users, used in fit\n",
    "    - n_rec: how many recommendations the system outputs\n",
    "    \n",
    "    \n",
    "    returns:\n",
    "    - two numpy arrays containing precision and recall\n",
    "    \"\"\"\n",
    "    precision= list()\n",
    "    recall= list()\n",
    "\n",
    "    M_rec = M.T.tocsr() #transpose to recommend\n",
    "    \n",
    "    print('Evaluating model...')\n",
    "    #calculate precision/recall for each user, append results to list\n",
    "    for user, holdout_items in tqdm(test.items()):\n",
    "        rec = model.recommend(user, M_rec, N=n_rec, filter_already_liked_items=True)\n",
    "        rec_items = [pair[0] for pair in rec]\n",
    "        \n",
    "        #count true positives in recommended items\n",
    "        tp = float(0)\n",
    "        for item in holdout_items: \n",
    "            if item in set(recs_items):\n",
    "                tp += 1\n",
    "                \n",
    "        #calculate precision and recall\n",
    "        precision.append(tp/n_rec) #fraction of recommendations that are in hold-out set\n",
    "        recall.append(tp/len(holdout_items)) #fraction of hold-out set that were reocmmended\n",
    "    \n",
    "    return np.asarray(precision), np.asarray(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TimGimi/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/TimGimi/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:12: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Sparsity: 99.9078520973\n",
      "Splitting train, test data for each user...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480b487552f44f8f9723aa7f8c8f2562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8848), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of original data masked: 0.256562846907\n",
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15.0/15 [00:05<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03062107302142e5896d16fc604664a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8178), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Precision: 1.9203961848862805 %\n",
      "Std of Precision: 4.497427023400114 %\n",
      "Mean of Recall: 3.845866997462742 %\n",
      "Std of Recall: 9.035758132828487 %\n"
     ]
    }
   ],
   "source": [
    "#MAIN SCRIPT\n",
    "\n",
    "plays_sparse = create_sparse_matrix(df)\n",
    "\n",
    "#filter out users with < 15 artists/reduce sparsity if needed\n",
    "print('Matrix Sparsity:', calculate_sparsity(plays_sparse))\n",
    "\n",
    "#split train,test by masking random values by user\n",
    "train, test = split_train_test_per_user(plays_sparse, 0.20, 20)\n",
    "print(\"Percentage of original data masked:\", pct_masked(plays_sparse, train))\n",
    "\n",
    "# Matrix Factorization\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "# model = implicit.nearest_neighbours.BM25Recommender()\n",
    "\n",
    "# train model \n",
    "print(\"Fitting model...\")\n",
    "model.fit(train, show_progress=True)\n",
    "\n",
    "precision, recall = evaluate(model, test, plays_sparse)\n",
    "print('Mean of Precision:',np.mean(precision)*100,'%')\n",
    "print('Std of Precision:',np.std(precision)*100,'%')\n",
    "print('Mean of Recall:',np.mean(recall)*100,'%')\n",
    "print('Std of Recall:',np.std(recall)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "#Primary metric: ROC-based\n",
    "#calculate Precision, Recall, TPr, FPr, AUC by comparing resulting matrix to test_data\n",
    "#Secondary metric: DCG\n",
    "\n",
    "#cross_validate(data, k)\n",
    "#get k-fold indices on train (mask again)\n",
    "#for each different k-fold, loop through the indices, masked as train, test as not\n",
    "#train model\n",
    "#calculate Precision, Recall, AUC, append result to list\n",
    "#return list of scores for each fold\n",
    "\n",
    "#calculate ATOP? DCG? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
