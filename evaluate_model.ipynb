{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#our goal is to have top K artist recommendations for a user, so that they would \n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import implicit\n",
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "df = pd.read_csv('lastfm_9000_users.csv', na_filter=False)\n",
    "df = df[df.plays > 10] #our threshold of relevance is 10 plays\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "BIG THINGS: \n",
    "- set-up cross-validation\n",
    "    - Baseline model evaluate\n",
    "    - Tune our Hyperparameters\n",
    "- fix precision/recall\n",
    "- calculate NDCG  \n",
    "- seeing if it scales\n",
    "\n",
    "CASES TO EVALUATE:\n",
    "- sparse vs. non-sparse data\n",
    "- threshold of relevance\n",
    "- which users should we hold out (those with > 10/20/30 values?)\n",
    "\n",
    "PERFORMANCE:\n",
    "- speed up split_train_per_user, by figuring out how to access zero values faster\n",
    "- speed up precision, recall evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create sparse matrix from dataframe object\n",
    "def create_sparse_matrix(data, user_user = True):\n",
    "    \"\"\"\n",
    "    Creates sparse matrix (csr_matrix) out of pandas dataframe.\n",
    "    \n",
    "    Parameters: \n",
    "    - data: Dataframe of user/artist data\n",
    "    - user_user: determines whether output will be for user-to-user or item-to-item collaborative filtering\n",
    "                 if user_user is True, then rows will be items and columns will be users\n",
    "    \n",
    "    Returns: \n",
    "    - plays_sparse: a sparse csr_matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Creating sparse matrix...\")\n",
    "    #grab unique users/artist IDS\n",
    "    users = list(np.sort(data.user_id.unique()))\n",
    "    artists = list(data.artist_name.unique())\n",
    "    plays = list(data.plays)\n",
    "\n",
    "    # user-user set-up\n",
    "    if (user_user == True):\n",
    "        rows = data.user_id.astype('category', categories=users).cat.codes\n",
    "        cols = data.artist_name.astype('category', categories=artists).cat.codes\n",
    "        plays_sparse = scipy.sparse.csr_matrix((plays, (rows, cols)), shape=(len(users),len(artists)))\n",
    "\n",
    "    #item-item set-up\n",
    "    else:    \n",
    "        rows = data.artist_name.astype('category', categories=artists).cat.codes\n",
    "        cols = data.user_id.astype('category', categories=users).cat.codes\n",
    "        plays_sparse = scipy.sparse.csr_matrix((plays, (rows, cols)), shape=(len(artists),len(users)))\n",
    "        \n",
    "    return plays_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate how sparse the matrix is\n",
    "def calculate_sparsity(M):\n",
    "    \"\"\"\n",
    "    Calculates how sparse this matrix\n",
    "    \"\"\"\n",
    "    matrix_size = float(M.shape[0]*M.shape[1]) # Number of possible interactions in the matrix\n",
    "    num_plays = len(M.nonzero()[0]) # Number of items interacted with\n",
    "    sparsity = 100*(1 - float(num_plays/matrix_size))\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split train, test using all user pairs\n",
    "def make_train_all_user_pairs(data, test_pct):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        data: data set in csr_matrix format\n",
    "        test_pct: percentage to be test set\n",
    "    \"\"\"\n",
    "    #create copies of dataset for training and test data\n",
    "    test = data.copy()\n",
    "    train = data.copy()\n",
    "    \n",
    "    #alter train data, masking/holding-out random user-pair values for some users\n",
    "    nonzero_idx = train.nonzero() #find indices in data where interaction exists\n",
    "    \n",
    "    \n",
    "    nonzero_pairs = zip(nonzero_idx[0], nonzero_idx[1]) #create pairs of (user, item) index\n",
    "    \n",
    "    #determine how many user-pair values we need to mask, according to test_pct\n",
    "    random.seed(0) #for reproducibility\n",
    "    num_samples = int(np.ceil(test_pct * len(nonzero_pairs)))\n",
    "    samples = random.sample(nonzero_pairs, num_samples) #sample random number of user-item pairs without replacement\n",
    "    print(type(samples))\n",
    "    #get user, item row and column indices\n",
    "    user_idx = [index[0] for index in samples] \n",
    "    item_idx = [index[1] for index in samples] \n",
    "    \n",
    "    train[user_idx,item_idx] = 0 #mask the randomly chosen user-item pairs\n",
    "    train.eliminate_zeros() #remove zeros in sparse arrays that was made previously\n",
    "    \n",
    "    return train, test, list(set(user_idx)), samples #output unique list of user rows that were altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train, test by user only with interactions#, with test size=total/k\n",
    "def split_train_test_per_user(data, k, interactions = 20,cross_valid= False,):\n",
    "    \"\"\"\n",
    "    Create train matrix with masked values and dictionary of test values \n",
    "    \n",
    "    Parameters:\n",
    "    - data: csr_matrix, assuming matrix is user-user (item as rows, columns as users)\n",
    "    - test_pct: percentage of items to mask per user\n",
    "    \n",
    "    Output:\n",
    "    - train: masked matrix\n",
    "    - test: list of tuples of held out data ((user_idx, item_idx), plays)\n",
    "    \"\"\"\n",
    "    random.seed(0) #for reproducibility\n",
    "    \n",
    "    train = data.copy() #transpose to make procedure easier/more intuitive\n",
    "    \n",
    "    test = dict() #dict to keep track of masked user-item values\n",
    "    \n",
    "    user_count = 0\n",
    "    test_list=[]\n",
    "    train_list=[]\n",
    "    if cross_valid==True: #initialize\n",
    "        for i in range(k):\n",
    "            test_list.append(dict())\n",
    "            train_list.append(train)\n",
    "    \n",
    "    #for each user in the training set\n",
    "    for user_idx in tqdm(range(train.get_shape()[0])):\n",
    "\n",
    "        #get indices of interactions of this user\n",
    "        nonzero_idx = train[user_idx].nonzero()\n",
    "\n",
    "        #only hold out users that have enough data (greater than interactions #)\n",
    "        if nonzero_idx[1].shape[0] >= interactions:\n",
    "            user_count += 1\n",
    "            #create list of tuples: interaction index (row, col) with the number of plays\n",
    "            nonzero_pairs = [((user_idx, item_idx), train[user_idx,item_idx]) for item_idx in nonzero_idx[1]]\n",
    "\n",
    "            #sort tuples descending by value\n",
    "            nonzero_sorted = sorted(nonzero_pairs, key = itemgetter(1), reverse = True)\n",
    "\n",
    "            #get top interaction # values, then sample test_pct% randomly from subset\n",
    "            top_values = nonzero_sorted[0:interactions]\n",
    "\n",
    "            #sample random number of item_indexes without replacement\n",
    "            num_samples = int(np.floor(interactions/float(k)))\n",
    "            if (cross_valid==False): \n",
    "                samples = random.sample(top_values, num_samples) \n",
    "\n",
    "                #append user_idx, item_\n",
    "                test[user_idx] = [pair[0][1] for pair in samples]\n",
    "\n",
    "                #mask the randomly chosen items of this user\n",
    "                for pair in samples:\n",
    "                    train[pair[0][0], pair[0][1]] = 0\n",
    "\n",
    "            else: #Cross Validation Step\n",
    "                for i in range(k):\n",
    "                    train = train_list[i]\n",
    "                    k_test=test_list[i]\n",
    "                    random.shuffle(top_values) \n",
    "                    samples=top_values[0:num_samples]\n",
    "                    top_values=top_values[num_samples:]\n",
    "                    #append user_idx, item_\n",
    "                    k_test[user_idx] = [pair[0][1] for pair in samples]\n",
    "                    test_list[i]=k_test #update test\n",
    "                    #mask the randomly chosen items of this user\n",
    "                    for pair in samples:\n",
    "                        train[pair[0][0], pair[0][1]] = 0\n",
    "                    train.eliminate_zeros()\n",
    "                    train_list[i]=train #update train\n",
    "    if (cross_valid==False):\n",
    "        return train.T.tocsr(), test, user_count #convert matrix back\n",
    "    else:\n",
    "        return train_list, test_list, user_count #convert matrix back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate how many interactions are masked compared to previous dataset\n",
    "def pct_masked(original, altered):\n",
    "    altered_n = altered.nonzero()[0].shape[0]\n",
    "    original_n = original.nonzero()[0].shape[0]\n",
    "    return (original_n - altered_n)/float(altered_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline(k,user_items):\n",
    "    plays=user_items.toarray()\n",
    "    totalplays=np.sum(plays,axis=1)    \n",
    "    idx = (-totalplays).argsort()[:k]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used to evaluate model\n",
    "def evaluate(model, test, M, n_rec = 20):\n",
    "    \"\"\"\n",
    "    Calculate precision/recall\n",
    "    \n",
    "    parameters:\n",
    "    - model: fitted implicit model that will perform recommendations\n",
    "    - test: list containing tuples that are heldout for each user\n",
    "    - M: csr_matrix of item-users, used in fit\n",
    "    - n_rec: how many recommendations the system outputs\n",
    "    \n",
    "    \n",
    "    returns:\n",
    "    - two numpy arrays containing precision and recall\n",
    "    \"\"\"\n",
    "    M_rec = M.T.tocsr() #transpose to recommend\n",
    "    \n",
    "    tp = float(0)\n",
    "    test_n = float(0)\n",
    "    print('Evaluating model...')\n",
    "    #calculate true positives for each user, append results to list\n",
    "    for user, holdout_items in tqdm(test.items()):\n",
    "        rec = model.recommend(user, M_rec, N=n_rec, filter_already_liked_items=True)\n",
    "        rec_items = [pair[0] for pair in rec]\n",
    "        test_n += len(holdout_items)\n",
    "        #count true positives in recommended items\n",
    "        \n",
    "        for item in holdout_items: \n",
    "            if item in rec_items:\n",
    "                tp += 1\n",
    "    \n",
    "    recall = tp/test_n\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used to evaluate model\n",
    "def evaluate_base(rec_items, test, M, n_rec = 20):\n",
    "    \"\"\"\n",
    "    Calculate recall\n",
    "    \n",
    "    parameters:\n",
    "    - model: fitted implicit model that will perform recommendations\n",
    "    - test: list containing tuples that are heldout for each user\n",
    "    - M: csr_matrix of item-users, used in fit\n",
    "    - n_rec: how many recommendations the system outputs\n",
    "    returns:\n",
    "    - numpy array containing recall\n",
    "    \"\"\"\n",
    "    M_rec = M.T.tocsr() #transpose to recommend\n",
    "    \n",
    "    tp = float(0)\n",
    "    test_n = float(0)\n",
    "    print('Evaluating model...')\n",
    "    #calculate true positives for each user, append results to list\n",
    "    for user, holdout_items in tqdm(test.items()):\n",
    "        test_n += len(holdout_items)\n",
    "        #count true positives in recommended items\n",
    "        for item in holdout_items: \n",
    "            if item in rec_items:\n",
    "                tp += 1\n",
    "    \n",
    "    recall = tp/test_n\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:24: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Sparsity: 99.90785209726101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9022ab77be414317b5b06acb0db7634c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of original data masked: 0.08944327630567989\n",
      "Users masked: 8178\n"
     ]
    }
   ],
   "source": [
    "#MAIN SCRIPT\n",
    "plays_sparse = create_sparse_matrix(df).astype('float')\n",
    "\n",
    "#filter out users with < 15 artists/reduce sparsity if needed\n",
    "print('Matrix Sparsity:', calculate_sparsity(plays_sparse))\n",
    "\n",
    "#split train,test by masking random values by user\n",
    "train, test, user_count = split_train_test_per_user(plays_sparse, 5,20)\n",
    "print(\"Percentage of original data masked:\", pct_masked(plays_sparse, train))\n",
    "print(\"Users masked:\", user_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15.0/15 [00:05<00:00,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f1f8b6e70146aa915bffd0694b04d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.82795304475422 %\n"
     ]
    }
   ],
   "source": [
    "# ALS Model-Based\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "# model = implicit.nearest_neighbours.BM25Recommender()\n",
    "\n",
    "# train model \n",
    "print(\"Fitting model...\")\n",
    "model.fit(train, show_progress=True)\n",
    "\n",
    "recall = evaluate(model, test, plays_sparse)\n",
    "print(recall*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BASELINE\n",
    "user_items = plays_sparse.T.tocsr()\n",
    "rec_items=baseline(20,user_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66236650c3a64fe8985e1d18c6473f4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.113964294448521 %\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Baseline\n",
    "recall = evaluate_base(rec_items,test,plays_sparse)\n",
    "print(recall*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2477d129b3648f2b6584e784f74a90c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cross Validation\n",
    "k=5\n",
    "train_list, test_list, user_count = split_train_test_per_user(plays_sparse,k,20,cross_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bf34abf60c43a2bb086d5211df52c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2515284910736115 %\n",
      "---------------------------\n",
      "Fitting model...\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fa03f7c12847b0ac27e026c856919e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.181217901687454 %\n",
      "---------------------------\n",
      "Fitting model...\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0fbd48a3b64e599e5213ee6a75553c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.202616776718024 %\n",
      "---------------------------\n",
      "Fitting model...\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86776a8a5bc64a4e81c9d5b2b9d8ccf4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1261922230374175 %\n",
      "---------------------------\n",
      "Fitting model...\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0876a85c786947f98cd172c61c292fea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.168989973098557 %\n",
      "---------------------------\n",
      "The mean recall is  0.06186109073123014\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Model-based from Cross Validation \n",
    "# ALS Model-Based\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "# model = implicit.nearest_neighbours.BM25Recommender()\n",
    "\n",
    "# train model\n",
    "recall_list=[]\n",
    "for i in range(k):\n",
    "    print(\"Fitting model...\")\n",
    "    train=train_list[i]\n",
    "    test=test_list[i]\n",
    "    model.fit(train, show_progress=False)\n",
    "    recall = evaluate(model,test,plays_sparse)\n",
    "    print(recall*100,'%')\n",
    "    recall_list.append(recall)\n",
    "    print(\"---------------------------\")\n",
    "print(\"The mean recall is \", np.mean(recall_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AUC https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "#Primary metric: ROC-based\n",
    "#calculate Precision, Recall, TPr, FPr, AUC by comparing resulting matrix to test_data\n",
    "#Secondary metric: DCG\n",
    "\n",
    "#cross_validate(data, k)\n",
    "#get k-fold indices on train (mask again)\n",
    "#for each different k-fold, loop through the indices, masked as train, test as not\n",
    "#train model\n",
    "#calculate Precision, Recall, AUC, append result to list\n",
    "#return list of scores for each fold\n",
    "\n",
    "#calculate ATOP? DCG? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8178"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_list[1].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8178"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
