{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our goal is to have top K artist recommendations for a user, so that they would \n",
    "from math import log\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import implicit\n",
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "df = pd.read_csv('lastfm_9000_users.csv', na_filter=False)\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "BIG THINGS: \n",
    "- set-up cross-validation\n",
    "    - Baseline model evaluate\n",
    "    - Tune our Hyperparameters\n",
    "- fix precision/recall\n",
    "- calculate NDCG  \n",
    "- seeing if it scales\n",
    "\n",
    "CASES TO EVALUATE:\n",
    "- sparse vs. non-sparse data\n",
    "- threshold of relevance\n",
    "- which users should we hold out (those with > 10/20/30 values?)\n",
    "\n",
    "PERFORMANCE:\n",
    "- speed up split_train_per_user, by figuring out how to access zero values faster\n",
    "- speed up precision, recall evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sparse matrix from dataframe object\n",
    "def create_sparse_matrix(data, user_user = True):\n",
    "    \"\"\"\n",
    "    Creates sparse matrix (csr_matrix) out of pandas dataframe.\n",
    "    \n",
    "    Parameters: \n",
    "    - data: Dataframe of user/artist data\n",
    "    - user_user: determines whether output will be for user-to-user or item-to-item collaborative filtering\n",
    "                 if user_user is True, then rows will be items and columns will be users\n",
    "    \n",
    "    Returns: \n",
    "    - plays_sparse: a sparse csr_matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Creating sparse matrix...\")\n",
    "    #grab unique users/artist IDS\n",
    "    users = list(np.sort(data.user_id.unique()))\n",
    "    artists = list(data.artist_mbid.unique())\n",
    "    plays = list(data.plays)\n",
    "\n",
    "    # user-user set-up\n",
    "    if (user_user == True):\n",
    "        rows = data.user_id.astype('category', categories=users).cat.codes\n",
    "        cols = data.artist_mbid.astype('category', categories=artists).cat.codes\n",
    "        plays_sparse = scipy.sparse.csr_matrix((plays, (rows, cols)), shape=(len(users),len(artists)))\n",
    "\n",
    "    #item-item set-up\n",
    "    else:    \n",
    "        rows = data.artist_mbid.astype('category', categories=artists).cat.codes\n",
    "        cols = data.user_id.astype('category', categories=users).cat.codes\n",
    "        plays_sparse = scipy.sparse.csr_matrix((plays, (rows, cols)), shape=(len(artists),len(users)))\n",
    "        \n",
    "    return plays_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate how sparse the matrix is\n",
    "def calculate_sparsity(M):\n",
    "    \"\"\"\n",
    "    Calculates how sparse this matrix\n",
    "    \"\"\"\n",
    "    matrix_size = float(M.shape[0]*M.shape[1]) # Number of possible interactions in the matrix\n",
    "    num_plays = len(M.nonzero()[0]) # Number of items interacted with\n",
    "    sparsity = 100*(1 - float(num_plays/matrix_size))\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train, test using all user pairs\n",
    "def make_train_all_user_pairs(data, test_pct):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        data: data set in csr_matrix format\n",
    "        test_pct: percentage to be test set\n",
    "    \"\"\"\n",
    "    #create copies of dataset for training and test data\n",
    "    test = data.copy()\n",
    "    train = data.copy()\n",
    "    \n",
    "    #alter train data, masking/holding-out random user-pair values for some users\n",
    "    nonzero_idx = train.nonzero() #find indices in data where interaction exists\n",
    "    \n",
    "    \n",
    "    nonzero_pairs = zip(nonzero_idx[0], nonzero_idx[1]) #create pairs of (user, item) index\n",
    "    \n",
    "    #determine how many user-pair values we need to mask, according to test_pct\n",
    "    random.seed(0) #for reproducibility\n",
    "    num_samples = int(np.ceil(test_pct * len(nonzero_pairs)))\n",
    "    samples = random.sample(nonzero_pairs, num_samples) #sample random number of user-item pairs without replacement\n",
    "    print(type(samples))\n",
    "    #get user, item row and column indices\n",
    "    user_idx = [index[0] for index in samples] \n",
    "    item_idx = [index[1] for index in samples] \n",
    "    \n",
    "    train[user_idx,item_idx] = 0 #mask the randomly chosen user-item pairs\n",
    "    train.eliminate_zeros() #remove zeros in sparse arrays that was made previously\n",
    "    \n",
    "    return train, test, list(set(user_idx)), samples #output unique list of user rows that were altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train, test by user only with interactions#, with test size=total/k\n",
    "def split_train_test_per_user(data, k, interactions = 20,cross_valid= False,):\n",
    "    \"\"\"\n",
    "    Create train matrix with masked values and dictionary of test values \n",
    "    \n",
    "    Parameters:\n",
    "    - data: csr_matrix, assuming matrix is user-user (item as rows, columns as users)\n",
    "    - test_pct: percentage of items to mask per user\n",
    "    \n",
    "    Output:\n",
    "    - train: masked matrix\n",
    "    - test: list of tuples of held out data ((user_idx, item_idx), plays)\n",
    "    \"\"\"\n",
    "    random.seed(0) #for reproducibility\n",
    "    \n",
    "    train = data.copy() #transpose to make procedure easier/more intuitive\n",
    "    \n",
    "    test = dict() #dict to keep track of masked user-item values\n",
    "    \n",
    "    user_count = 0\n",
    "    test_list=[]\n",
    "    train_list=[]\n",
    "    if cross_valid==True: #initialize\n",
    "        for i in range(k):\n",
    "            test_list.append(dict())\n",
    "            train_list.append(train)\n",
    "    \n",
    "    #for each user in the training set\n",
    "    for user_idx in tqdm(range(train.get_shape()[0])):\n",
    "\n",
    "        #get indices of interactions of this user\n",
    "        nonzero_idx = train[user_idx].nonzero()\n",
    "\n",
    "        #only hold out users that have enough data (greater than interactions #)\n",
    "        if nonzero_idx[1].shape[0] >= interactions:\n",
    "            user_count += 1\n",
    "            #create list of tuples: interaction index (row, col) with the number of plays\n",
    "            nonzero_pairs = [((user_idx, item_idx), train[user_idx,item_idx]) for item_idx in nonzero_idx[1]]\n",
    "\n",
    "            #sort tuples descending by value\n",
    "            nonzero_sorted = sorted(nonzero_pairs, key = itemgetter(1), reverse = True)\n",
    "\n",
    "            #get top interaction # values, then sample test_pct% randomly from subset\n",
    "            top_values = nonzero_sorted[0:interactions]\n",
    "\n",
    "            #sample random number of item_indexes without replacement\n",
    "            num_samples = int(np.floor(interactions/float(k)))\n",
    "            if (cross_valid==False): \n",
    "                samples = random.sample(top_values, num_samples) \n",
    "\n",
    "                #append user_idx, item_\n",
    "                test[user_idx] = [pair[0][1] for pair in samples]\n",
    "\n",
    "                #mask the randomly chosen items of this user\n",
    "                for pair in samples:\n",
    "                    train[pair[0][0], pair[0][1]] = 0\n",
    "\n",
    "            else: #Cross Validation Step\n",
    "                for i in range(k):\n",
    "                    train = train_list[i]\n",
    "                    k_test=test_list[i]\n",
    "                    random.shuffle(top_values) \n",
    "                    samples=top_values[0:num_samples]\n",
    "                    top_values=top_values[num_samples:]\n",
    "                    #append user_idx, item_\n",
    "                    k_test[user_idx] = [pair[0][1] for pair in samples]\n",
    "                    test_list[i]=k_test #update test\n",
    "                    #mask the randomly chosen items of this user\n",
    "                    for pair in samples:\n",
    "                        train[pair[0][0], pair[0][1]] = 0\n",
    "                    train.eliminate_zeros()\n",
    "                    train_list[i]=train #update train\n",
    "    if (cross_valid==False):\n",
    "        return train.T.tocsr(), test, user_count #convert matrix back\n",
    "    else:\n",
    "        for i in range(k):\n",
    "            train_list[i]=train_list[i].T.tocsr()\n",
    "        return train_list, test_list, user_count #convert matrix back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate how many interactions are masked compared to previous dataset\n",
    "def pct_masked(original, altered):\n",
    "    altered_n = altered.nonzero()[0].shape[0]\n",
    "    original_n = original.nonzero()[0].shape[0]\n",
    "    return (original_n - altered_n)/float(altered_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(k,user_items):\n",
    "    plays=user_items.toarray()\n",
    "    totalplays=np.sum(plays,axis=1)    \n",
    "    idx = (-totalplays).argsort()[:k]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation/Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_list(n):\n",
    "    listofzeros = [0] * n\n",
    "    return listofzeros\n",
    "\n",
    "def dcg_at_k(scores):\n",
    "    assert scores\n",
    "    return scores[0] + sum(sc / log(ind, 2) for sc, ind in zip(scores[1:], range(2, len(scores)+1)))\n",
    "\n",
    "def ndcg_at_k(predicted_scores, user_scores):\n",
    "    \"\"\"\n",
    "    predicted_scores: recommended k items from model\n",
    "    user_scores: held out items\n",
    "    \"\"\"\n",
    "    assert len(predicted_scores) == len(user_scores)\n",
    "\n",
    "    idcg = dcg_at_k(sorted(user_scores, reverse=True))\n",
    "    x = (dcg_at_k(predicted_scores) / idcg) if idcg > 0.0 else 0.0\n",
    "    \n",
    "    return x\n",
    "\n",
    "#used to evaluate model\n",
    "def evaluate(model, test, M, n_rec = 20):\n",
    "    \"\"\"\n",
    "    Calculate precision/recall\n",
    "    \n",
    "    parameters:\n",
    "    - model: fitted implicit model that will perform recommendations\n",
    "    - test: list containing tuples that are heldout for each user\n",
    "    - M: csr_matrix of item-users, used in fit\n",
    "    - n_rec: how many recommendations the system outputs\n",
    "    \n",
    "    \n",
    "    returns:\n",
    "    - two numpy arrays containing precision and recall\n",
    "    \"\"\"\n",
    "    #TODO: Refactor NDCG and Recall, less dependent on each other\n",
    "    \n",
    "    M_rec = M.T.tocsr() #transpose to recommend\n",
    "    \n",
    "    tp = float(0)\n",
    "    test_n = float(0)\n",
    "    print('Evaluating model...')\n",
    "    \n",
    "    ndcg = list()\n",
    "    #calculate true positives for each user, append results to list\n",
    "    for user, holdout_items in tqdm(test.items()):\n",
    "        \n",
    "        #get list of item recs for each user\n",
    "        rec = model.recommend(user, M_rec, N=n_rec, filter_already_liked_items=True) #returns (item_id, score)\n",
    "        rec_items = [pair[0] for pair in rec] #get only item_id\n",
    "        test_n += len(holdout_items) #total number of heldout items\n",
    "        \n",
    "        #for NDCG\n",
    "        predicted_scores = zeros_list(n_rec)\n",
    "        user_scores = zeros_list(n_rec)\n",
    "        i = 0\n",
    "            \n",
    "        #count true positives in recommended items\n",
    "        for item in holdout_items: \n",
    "            value = M[user,item]\n",
    "            user_scores[i] = value\n",
    "            i += 1\n",
    "            if item in rec_items:\n",
    "                predicted_scores[rec_items.index(item)] = value #if holdout items is in recommended\n",
    "                tp += 1\n",
    "        #Calculate NDCG\n",
    "        ndcg.append(ndcg_at_k(predicted_scores, user_scores))\n",
    "        \n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "    recall = tp/test_n\n",
    "    return recall, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to evaluate model\n",
    "def evaluate_base(rec_items, test, M, n_rec = 20):\n",
    "    \"\"\"\n",
    "    Calculate recall\n",
    "    \n",
    "    parameters:\n",
    "    - model: fitted implicit model that will perform recommendations\n",
    "    - test: list containing tuples that are heldout for each user\n",
    "    - M: csr_matrix of item-users, used in fit\n",
    "    - n_rec: how many recommendations the system outputs\n",
    "    returns:\n",
    "    - numpy array containing recall\n",
    "    \"\"\"\n",
    "    M_rec = M.T.tocsr() #transpose to recommend\n",
    "    \n",
    "    tp = float(0)\n",
    "    test_n = float(0)\n",
    "    rec_items = list(rec_items)\n",
    "    print('Evaluating model...')\n",
    "    #calculate true positives for each user, append results to list\n",
    "    \n",
    "    \n",
    "    for user, holdout_items in tqdm(test.items()):\n",
    "        test_n += len(holdout_items)\n",
    "        #count true positives in recommended items\n",
    "        predicted_scores = zeros_list(n_rec)\n",
    "        user_scores = zeros_list(n_rec)\n",
    "        i = 0\n",
    "        for item in holdout_items:\n",
    "            value = M[user,item]\n",
    "            user_scores[i] = value\n",
    "            i += 1\n",
    "            if item in rec_items:\n",
    "                predicted_scores[rec_items.index(item)] = value #if holdout items is in recommended\n",
    "                tp += 1\n",
    "\n",
    "        ndcg.append(ndcg_at_k(predicted_scores, user_scores))\n",
    "        \n",
    "    \n",
    "    recall = tp/test_n\n",
    "    return ndcg, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Script\n",
    "\n",
    "#### Create sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sparse matrix...\n",
      "Matrix Sparsity: 99.90909366166659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b244ef61986c45c7a0787ea5f3af9424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of original data masked: 0.06548307746157062\n",
      "Users masked: 8980\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PREPARE\n",
    "\"\"\"\n",
    "#create sparse matrix\n",
    "plays_sparse = create_sparse_matrix(df).astype('float')\n",
    "\n",
    "#filter out users with < 15 artists/reduce sparsity if needed\n",
    "print('Matrix Sparsity:', calculate_sparsity(plays_sparse))\n",
    "\n",
    "\"\"\"\n",
    "SPLIT TRAIN TEST\n",
    "\"\"\"\n",
    "train, test, user_count = split_train_test_per_user(plays_sparse, 3, 10)\n",
    "print(\"Percentage of original data masked:\", pct_masked(plays_sparse, train))\n",
    "print(\"Users masked:\", user_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model-based once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15.0/15 [00:03<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74018c4f18c24fbd98f09cda3ff4b4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall: 21.638869313287916 %\n",
      "Average NDCG: 11.941251293622726 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALS MODEL BASED\n",
    "\"\"\"\n",
    "model = implicit.als.AlternatingLeastSquares(factors=30)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "# model = implicit.nearest_neighbours.BM25Recommender()\n",
    "\n",
    "# train model \n",
    "print(\"Fitting model...\")\n",
    "model.fit(train, show_progress=True)\n",
    "\n",
    "recall, ndcg = evaluate(model, test, plays_sparse)\n",
    "print(\"Recall:\",recall*100,'%')\n",
    "print(\"Average NDCG:\",np.mean(ndcg)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Baseline once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASELINE\n",
    "user_items = plays_sparse.T.tocsr()\n",
    "rec_items=baseline(20,user_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c90562ee9c645dd80f558f75e284c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7.3643410852713185 %\n",
      "Average NDCG per User: 8.156478921754486 %\n"
     ]
    }
   ],
   "source": [
    "#Evaluate Baseline\n",
    "ndcg, recall = evaluate_base(rec_items,test,plays_sparse)\n",
    "print(recall*100,'%')\n",
    "print(\"Average NDCG per User:\",np.mean(ndcg)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7089a5f4f34005833326ad7e25e1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "k=5\n",
    "train_list, test_list, user_count = split_train_test_per_user(plays_sparse,k,20,cross_valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e531c6d3444e8f9e39e9e658093545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a301050ba51e4e4097573a660506aa5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595c6b9220044c5781c9ecf48923a08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd859d66a3a849008c116563ed796037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129f9794cbfc4eb09f53adfb90d8fd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-de5e7e92510c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrecall_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The mean recall is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2956\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2957\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#Evaluate Model-based from Cross Validation \n",
    "\n",
    "# train model\n",
    "recall_list=[]\n",
    "for i in range(k):\n",
    "    print(\"Fitting model...\")\n",
    "    train=train_list[i]\n",
    "    test=test_list[i]\n",
    "    model = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "    model.fit(train, show_progress=False)\n",
    "    recall = evaluate(model,test,plays_sparse)\n",
    "    print(recall*100,'%')\n",
    "    recall_list.append(recall)\n",
    "    print(\"---------------------------\")\n",
    "print(\"The mean recall is \", np.mean(recall_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate KNN from Cross Validation \n",
    "\n",
    "# train model\n",
    "recall_list=[]\n",
    "for i in range(k):\n",
    "    print(\"Fitting model...\")\n",
    "    train=train_list[i]\n",
    "    test=test_list[i]\n",
    "    model = implicit.nearest_neighbours.BM25Recommender()    \n",
    "    model.fit(train, show_progress=False)\n",
    "    recall = evaluate(model,test,plays_sparse)\n",
    "    print(recall*100,'%')\n",
    "    recall_list.append(recall)\n",
    "    print(\"---------------------------\")\n",
    "print(\"The mean recall is \", np.mean(recall_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate Model-based from Baseline\n",
    "\n",
    "# train model\n",
    "recall_list=[]\n",
    "for i in range(k):\n",
    "    print(\"Fitting model...\")\n",
    "    train=train_list[i]\n",
    "    test=test_list[i]\n",
    "    user_items = plays_sparse.T.tocsr()\n",
    "    rec_items=baseline(20,user_items)\n",
    "    recall = evaluate_base(rec_items,test,plays_sparse)\n",
    "    print(recall*100,'%')\n",
    "    recall_list.append(recall)\n",
    "    print(\"---------------------------\")\n",
    "print(\"The mean recall is \", np.mean(recall_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "model.fit(plays_sparse)\n",
    "catalog = []\n",
    "for i in range(0,len(users)):\n",
    "    for x,y in model.recommend(i,plays_sparse.T.tocsr(), N=20, filter_already_liked_items=True):\n",
    "        if x not in catalog:\n",
    "            catalog.append(x)\n",
    "print('Catalog Coverage is', len(catalog)/len(artists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "#Primary metric: ROC-based\n",
    "#calculate Precision, Recall, TPr, FPr, AUC by comparing resulting matrix to test_data\n",
    "#Secondary metric: DCG\n",
    "\n",
    "#cross_validate(data, k)\n",
    "#get k-fold indices on train (mask again)\n",
    "#for each different k-fold, loop through the indices, masked as train, test as not\n",
    "#train model\n",
    "#calculate Precision, Recall, AUC, append result to list\n",
    "#return list of scores for each fold\n",
    "\n",
    "#calculate ATOP? DCG? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
